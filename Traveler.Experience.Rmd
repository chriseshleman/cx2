---
title: "Traveler Experience"
author: "Christopher Eshleman"
date: "11/8/2019"
output: pdf_document
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=FALSE)
```

\pagenumbering{arabic} 

#JD Power data 

We've spent a little time with the ASQ and JD Power data sets. A brief discussion follows. 

It's worth asking this question up front: what's a primary goal of this exploration (or subsequent correlation analysis)? Is it to understand market segmentation at the airports? Or is it to find strong predictors of overall satisfaction? 

We can address that later. For now, we see a number of files in the folders. But there are really two critical raw data files: 

1. One ASQ raw data file from 2019 (ACI ASQ Survey Main_ Q3 2019 Data-EXCEL-v1.xlsx), and,
2. One JD Power raw data file from 2018 (Client_SPSS_File_W1W4 (1).csv). 

Everything else is a PDF report or, in a couple of cases, Excel files that include synthesized tables or data dictionaries. That's as of the morning of 11/5. 

We started with those two data files. In particular, we were interested in the JD Power data since it's potentially so rich and provides a new data source outside of ASQ. 

```{r}
setwd("~/Dropbox/Work and research/Port Authority/cx2")
library(readxl) 
library(mice) 
library(dplyr) 
library(Hmisc) 
library(imputeMissings) 
library(ggplot2)
library(beepr) 
library(ggdendro) 
library(reshape2) 
library(tidyr) 

rm(list = ls()) # clear global environment 
cat("\014") # clear the console 
options(warn=-1) # suppress annoying warnings 
```


The ASQ data from ACI is relatively clean. 
```{r}
asq193 = read_excel("./ACI - ASQ/2019 Q3/ACI ASQ Survey Main_ Q3 2019 Data-EXCEL-v1.xlsx")
asq193 = as.data.frame(asq193) 
names(asq193) = tolower(names(asq193)) 
```

The JD Power data set is rougher, but potentially very rich. It has more than 400 fields and around 40,000 observations taken from four survey visits (multiple days) each in late 2017 through fall 2018. The data "labels" are stored in a separate file with different formatting so we've transposed them and glued them to the master data set. 

```{r}
jd18 = read.csv("./JD Power/2018 Study/Raw Data/Client_SPSS_File_W1W4 (1).csv")
table(jd18$SURVEY_MONTH, jd18$SURVEY_YEAR) 
jdnames = read_excel("./JD Power/2018 Study/Raw Data/2018 W1-W4 Data Dictionary.xlsx", skip = 1) 
jdnames = as.data.frame(jdnames) 
jdnames = jdnames[,c(1,3)] # Pick only the field names and labels. Labels will be important later.
jdnames = as.data.frame(gsub("[[:punct:]]", "", as.matrix(jdnames))) # Remove special characters.
# Remove spaces in names, transpose them, and take the labels only for short-term use.
jdnames$Label = gsub(" ",".",jdnames$Label) 
jdnames.t = t(jdnames) 
jdlabels = jdnames.t[2,] # Take the second row only (labels) 
# Combine JD Power labels with data. 
jdlabels = head(jdlabels,411) # Only the first 411 rows count 
names(jd18) = c(jdlabels) 
```

We're guessing "Overall.Satisfaction.Index" is the variable of interest. It appears to be scaled from, roughly, 100-1000, which seems to match some of the visualizations in the PDF reports:

```{r, echo=FALSE, include=TRUE}
summary(jd18$Overall.Satisfaction.Index) 
```

The JD Power data has 411 variables. We dropped 26 that promised little value or would actually get in the way of correlation analysis. 

```{r}
# Take a hint from https://uc-r.github.io/hc_clustering 
jd18.2 = jd18[,-(1:7)] 
#jd18.2.2 = jd18.2[,101:200] 
#jd18.2.3 = jd18.2[201:300]
#jd18.2.4 = jd18.2[301:398]
#str(jd18.2.2) 
#str(jd18.2.3) 
#str(jd18.2.4) 
jd18.2$Departure.flight..Travel.Dates = NULL 
jd18.2$Arrival.flight..Travel.Dates = NULL 
jd18.2$ZipPostal.code = NULL 
jd18.2$MRPSURVEYDPSTACKID = NULL 
jd18.2$AF7.Verbatim = NULL 
jd18.2$Why.public.transportation.not.used = NULL 
jd18.2$What.foodBeverages.want.to.find = NULL 
jd18.2$F15.Verbatim = NULL 
jd18.2$RS6.Verbatim = NULL 
jd18.2$TF2B.Verbatim = NULL 
jd18.2$YF2.Verbatim = NULL 
jd18.2$YF2.Verbatim.1 = NULL 
jd18.2$L7.Verbatim = NULL 
jd18.2$Reason.for.NPS.rating = NULL 
jd18.2$Other.9 = NULL 
jd18.2$FB2.Verbatim = NULL 
jd18.2$RS196.Verbatim = NULL 
jd18.2$RS197.Verbatim = NULL 
jd18.2$Regarding.the.cleanliness.of.the.terminal.why.did.you.provide.a.rating.of.MRKTF21B.for.MRKAIRPORT = NULL 

not_all_na = function(x) {!all(is.na(x))} # Quick function 
jd18.2 = jd18.2 %>% select_if(not_all_na) # Remove variables with all NAs. 
```

Here are the variable names after re-labeling and a quick summary of each. Notice the number of missing observations (NAs) and the variation in those missing observations' propensity across different variables. And, to make matters tougher, the missing observations' are distributed variously across survey respondents - they're not limited to certain questions. So when you try and just drop respondents with any NA answers or, conversely, drop survey questions with any NA responses, you're left with nothing. 

This could present real issues - let's chat before we do more. 

##Variable names (labels):

```{r, echo=FALSE, include=TRUE}
names(jd18.2) 
```

##Summaries: 
```{r, echo=FALSE, include=TRUE}
summary(jd18.2) 
```

