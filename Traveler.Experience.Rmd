---
title: "Traveler Experience"
author: "Planning and Regional Development"
date: "11/13/2019"
output: pdf_document
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=FALSE)
```

\pagenumbering{arabic} 

#JD Power data 

We've spent a little time with the ASQ and JD Power data sets. A brief discussion follows. 

It's worth asking this question up front: what's a primary goal of this exploration (or subsequent correlation analysis)? Is it to understand market segmentation at the airports? Or is it to find strong predictors of overall satisfaction? 

We can address that later. For now, we see a number of files in the folders. But there are really two critical raw data files: 

1. One ASQ raw data file from 2019 (ACI ASQ Survey Main_ Q3 2019 Data-EXCEL-v1.xlsx), and,
2. One JD Power raw data file from 2018 (Client_SPSS_File_W1W4 (1).csv). 

Everything else is a PDF report or, in a couple of cases, Excel files that include synthesized tables or data dictionaries. That's as of the morning of 11/5. 

We started with those two data files. In particular, we were interested in the JD Power data since it's potentially so rich and provides a new data source outside of ASQ. 

```{r}
setwd("~/Dropbox/Work and research/Port Authority/cx2")
library(readxl) 
library(mice) 
library(dplyr) 
library(Hmisc) 
library(imputeMissings) 
library(ggplot2)
library(beepr) 
library(ggdendro) 
library(reshape2) 
library(tidyr) 
#install.packages("nanair", dependencies=TRUE, repos='http://cran.rstudio.com/')
#install.packages("remotes") 
library(remotes) 
#remotes::install_github("njtierney/naniar") 
library(naniar) 

rm(list = ls()) # clear global environment 
cat("\014") # clear the console 
options(warn=-1) # suppress annoying warnings 
```


The ASQ data from ACI is relatively clean. 
```{r}
asq193 = read_excel("./ACI - ASQ/2019 Q3/ACI ASQ Survey Main_ Q3 2019 Data-EXCEL-v1.xlsx")
asq193 = as.data.frame(asq193) 
names(asq193) = tolower(names(asq193)) 
```

The JD Power data set is rougher, but potentially very rich. It has more than 400 fields and around 40,000 observations taken from four survey visits (multiple days) each in late 2017 through fall 2018. The data "labels" are stored in a separate file with different formatting so we've transposed them and glued them to the master data set. 

```{r}
jd18 = read.csv("./JD Power/2018 Study/Raw Data/Client_SPSS_File_W1W4 (1).csv")
table(jd18$SURVEY_MONTH, jd18$SURVEY_YEAR) 
jdnames = read_excel("./JD Power/2018 Study/Raw Data/2018 W1-W4 Data Dictionary.xlsx", skip = 1) 
jdnames = as.data.frame(jdnames) 
jdnames = jdnames[,c(1,3)] # Pick only the field names and labels. Labels will be important later.
jdnames = as.data.frame(gsub("[[:punct:]]", "", as.matrix(jdnames))) # Remove special characters.
# Remove spaces in names, transpose them, and take the labels only for short-term use.
jdnames$Label = gsub(" ",".",jdnames$Label) 
jdnames.t = t(jdnames) 
jdlabels = jdnames.t[2,] # Take the second row only (labels) 
# Combine JD Power labels with data. 
jdlabels = head(jdlabels,411) # Only the first 411 rows count 
names(jd18) = c(jdlabels) 
```

We're guessing "Overall.Satisfaction.Index" is the variable of interest. It appears to be scaled from, roughly, 100-1000, which seems to match some of the visualizations in the PDF reports:

```{r, echo=FALSE, include=TRUE}
summary(jd18$Overall.Satisfaction.Index)
```

The JD Power data has 411 variables. We dropped 26 that promised little value or would actually get in the way of correlation analysis. 

```{r}
# Take a hint from https://uc-r.github.io/hc_clustering 
jd18.2 = jd18[,-(1:7)] 
#jd18.2.2 = jd18.2[,101:200] 
#jd18.2.3 = jd18.2[201:300]
#jd18.2.4 = jd18.2[301:398]
#str(jd18.2.2) 
#str(jd18.2.3) 
#str(jd18.2.4) 
jd18.2$Departure.flight..Travel.Dates = NULL 
jd18.2$Arrival.flight..Travel.Dates = NULL 
jd18.2$ZipPostal.code = NULL 
jd18.2$MRPSURVEYDPSTACKID = NULL 
jd18.2$AF7.Verbatim = NULL 
jd18.2$Why.public.transportation.not.used = NULL 
jd18.2$What.foodBeverages.want.to.find = NULL 
jd18.2$F15.Verbatim = NULL 
jd18.2$RS6.Verbatim = NULL 
jd18.2$TF2B.Verbatim = NULL 
jd18.2$YF2.Verbatim = NULL 
jd18.2$YF2.Verbatim.1 = NULL 
jd18.2$L7.Verbatim = NULL 
jd18.2$Reason.for.NPS.rating = NULL 
jd18.2$Other.9 = NULL 
jd18.2$FB2.Verbatim = NULL 
jd18.2$RS196.Verbatim = NULL 
jd18.2$RS197.Verbatim = NULL 
jd18.2$Regarding.the.cleanliness.of.the.terminal.why.did.you.provide.a.rating.of.MRKTF21B.for.MRKAIRPORT = NULL 

not_all_na = function(x) {!all(is.na(x))} # Quick function 
jd18.2 = jd18.2 %>% select_if(not_all_na) # Remove variables with all NAs. 
```

Here are the variable names after re-labeling and a quick summary of each. Notice the number of missing observations (NAs) and the variation in those missing observations' propensity across different variables. And, to make matters tougher, the missing observations' are distributed variously across survey respondents - they're not limited to certain questions. So when you try and just drop respondents with any NA answers or, conversely, drop survey questions with any NA responses, you're left with nothing. 

(A list of varible names (labels) and summary statistics takes up the lion's share of this output, starting in a couple of pages.) 

##Understanding the missing data. 
Is the data missing at random? (Hopefully.) And how much is missing? 

Assuming the missing data is missing at random, we could impute some data points if NAs represent only a fraction of the total. Variables where the share of NAs are too large should probably be omitted.

Check for variables where much of the data is missing using a simple function. 

```{r, echo=TRUE}
pmiss = function(x){sum(is.na(x))/length(x)*100} 
# Taking a tip here from https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/ 
```


Take a random-ish sample of 12% of the variables. (Just pick every 8th column.)
Then take a truly random sample of 4,000 observations, which is around 10% of the full list of respondents.  
```{r}
jd18.2.small = jd18.2[, seq(1, ncol(jd18.2), 8)]
jd18.2.small = jd18.2.small[sample(nrow(jd18.2.small), 4000), ] # 4000 random rows 
```

Take a look for patterns in the missing observations. Here's a visualization: 

```{r, include=TRUE, fig.width=8, fig.height=10}
gg_miss_var(jd18.2.small, show_pct=TRUE)
#Kudos to Nick Tierney for the tips: https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html.
```

This stuff definitely isn't missing at random. Some variables are missing around 20 percent of observations - and that number looks very consistant across that group of variables. Then the share of missing observations increases, but it looks like it does it in a stepwise fashion - two or three variables at a time will have exactly the same number of missing observations. This continues all the way up until a collection of variables show zero actual measurements - all observations there are missing. 

A check of the data confirms that the first cluster of variables are all missing exactly 21.200 percent of their data. 

```{r, echo=TRUE, include=FALSE}
apply(jd18.2.small,2,pmiss)
```

That's way too consistent to be missing at random. 

So we can evaluate NAs further or we can limit the data set to everything missing no more than that much data, then do some strategic selection or imputation and move forward. 

```{r, fig.width=8, fig.height=10}
vis_miss(jd18.2.small)
```


##Next step: 
Plot the number of missing observations in a variable grouped by another variable using the facet argument. This would shed light on whether they are missing by survey month - perhaps one question was asked during the April survey but not the other three surveys. Or maybe it varied by the person conducting the survey. Those are the two potential answers that come to mind. 

```{r, echo=FALSE, include=FALSE}
gg_miss_var(airquality,
            facet = Month)
```


#Appendix 

##Variable names (labels):

```{r, echo=FALSE, include=TRUE}
names(jd18.2) 
```

##Summaries: 
```{r, echo=FALSE, include=TRUE}
summary(jd18.2) 
```

```{r}
options(warn=0) # turn annoying warnings back on
```

